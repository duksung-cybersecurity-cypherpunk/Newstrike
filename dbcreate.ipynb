{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daun/miniconda3/envs/rag/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 10 fields in line 6, saw 12\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 130\u001b[0m\n\u001b[1;32m    128\u001b[0m pdf_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./pdf1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    129\u001b[0m csv_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./pdf3CSV\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 130\u001b[0m \u001b[43mprocess_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# Step 5: Search Functionality\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch\u001b[39m(query, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# Semantic Search (Chromadb)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 122\u001b[0m, in \u001b[0;36mprocess_folder\u001b[0;34m(pdf_folder, csv_folder)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    121\u001b[0m         csv_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(csv_folder, filename)\n\u001b[0;32m--> 122\u001b[0m         csv_data \u001b[38;5;241m=\u001b[39m \u001b[43mextract_data_from_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m         all_data\u001b[38;5;241m.\u001b[39mextend(csv_data)\n\u001b[1;32m    125\u001b[0m index_data(all_data)\n",
      "Cell \u001b[0;32mIn[1], line 48\u001b[0m, in \u001b[0;36mextract_data_from_csv\u001b[0;34m(csv_path)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_data_from_csv\u001b[39m(csv_path):\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"CSV에서 텍스트를 추출하여 반환합니다.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     text_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.12/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 10 fields in line 6, saw 12\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import chromadb\n",
    "from chromadb import Settings\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # LangChain의 텍스트 스플리터\n",
    "from whoosh.index import create_in, open_dir\n",
    "from whoosh.fields import Schema, TEXT, ID\n",
    "from whoosh.qparser import QueryParser\n",
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "import uuid  # For generating unique IDs\n",
    "\n",
    "# Set up OpenAI API key using environment variables\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"yourapikey\"\n",
    "\n",
    "# Initialize OpenAI Embeddings\n",
    "openai_embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# Step 1: Data Preprocessing - PDF and CSV Handling with LangChain's RecursiveCharacterTextSplitter\n",
    "def chunk_text(text, chunk_size=300, chunk_overlap=50):\n",
    "    \"\"\"LangChain의 RecursiveCharacterTextSplitter로 텍스트를 분할합니다.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    return text_splitter.split_text(text)\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"PDF에서 텍스트를 추출하고 분할하여 반환합니다.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text_data = []\n",
    "    for page_num in range(doc.page_count):\n",
    "        page = doc[page_num]\n",
    "        text = page.get_text(\"text\")\n",
    "        chunks = chunk_text(text)  # LangChain의 텍스트 분할 사용\n",
    "        for chunk_num, chunk in enumerate(chunks, 1):\n",
    "            text_data.append({\n",
    "                \"content\": chunk,\n",
    "                \"page_num\": page_num + 1,\n",
    "                \"chunk_num\": chunk_num,\n",
    "                \"source\": pdf_path\n",
    "            })\n",
    "    return text_data\n",
    "\n",
    "def extract_data_from_csv(csv_path):\n",
    "    \"\"\"CSV에서 텍스트를 추출하여 반환합니다.\"\"\"\n",
    "    print(f\"Reading CSV file: {csv_path}\")  # 파일 경로 출력\n",
    "    df = pd.read_csv(csv_path)\n",
    "    text_data = []\n",
    "    for i, row in df.iterrows():\n",
    "        row_text = row.to_string()\n",
    "        text_data.append({\n",
    "            \"content\": row_text,\n",
    "            \"row_num\": i + 1,\n",
    "            \"source\": csv_path\n",
    "        })\n",
    "    return text_data\n",
    "\n",
    "# Step 2: Initialize Chromadb with persistent storage\n",
    "chroma_client = chromadb.PersistentClient(\n",
    "    path=\"./chroma1105\",\n",
    "    settings=Settings(allow_reset=True)\n",
    ")\n",
    "collection = chroma_client.get_or_create_collection(\"document_collection\")\n",
    "\n",
    "# Step 3: Initialize Whoosh for BM25 with persistent storage\n",
    "schema = Schema(content=TEXT(stored=True), source=ID(stored=True), page_or_row_num=ID(stored=True), chunk_num=ID(stored=True))\n",
    "index_dir = \"indexdir\"\n",
    "if os.path.exists(index_dir):\n",
    "    import shutil\n",
    "    shutil.rmtree(index_dir)\n",
    "os.mkdir(index_dir)\n",
    "ix = create_in(index_dir, schema)\n",
    "\n",
    "def index_data(data):\n",
    "    \"\"\"데이터를 Chroma와 Whoosh 인덱스에 추가합니다.\"\"\"\n",
    "    with ix.writer() as writer:\n",
    "        for entry in data:\n",
    "            unique_id = str(uuid.uuid4())\n",
    "            \n",
    "            # Embedding for Chromadb\n",
    "            embedding = openai_embeddings.embed_query(entry[\"content\"])\n",
    "            metadata = {\n",
    "                \"source\": entry[\"source\"], \n",
    "                \"page_or_row_num\": entry[\"page_num\" if \"page_num\" in entry else \"row_num\"]\n",
    "            }\n",
    "            \n",
    "            # CSV 데이터의 경우 chunk_num을 생략하거나 기본값 설정\n",
    "            if \"chunk_num\" in entry:\n",
    "                metadata[\"chunk_num\"] = entry[\"chunk_num\"]\n",
    "            \n",
    "            collection.add(\n",
    "                ids=[unique_id],\n",
    "                embeddings=[embedding],\n",
    "                documents=[entry[\"content\"]],\n",
    "                metadatas=[metadata]\n",
    "            )\n",
    "            \n",
    "            # Index for BM25 (Whoosh)\n",
    "            writer.add_document(\n",
    "                content=entry[\"content\"],\n",
    "                source=entry[\"source\"],\n",
    "                page_or_row_num=str(entry[\"page_num\" if \"page_num\" in entry else \"row_num\"]),\n",
    "                chunk_num=str(entry.get(\"chunk_num\", 1))  # chunk_num이 없으면 기본값 1로 설정\n",
    "            )\n",
    "            print(f\"Indexed document: {entry['content'][:30]}...\")  # Debug: Show part of the indexed document\n",
    "\n",
    "\n",
    "# Step 4: Process all PDF and CSV files in folders\n",
    "def process_folder(pdf_folder, csv_folder):\n",
    "    all_data = []\n",
    "    \n",
    "    for filename in os.listdir(pdf_folder):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(pdf_folder, filename)\n",
    "            pdf_data = extract_text_from_pdf(pdf_path)\n",
    "            all_data.extend(pdf_data)\n",
    "    \n",
    "    for filename in os.listdir(csv_folder):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            csv_path = os.path.join(csv_folder, filename)\n",
    "            csv_data = extract_data_from_csv(csv_path)\n",
    "            all_data.extend(csv_data)\n",
    "    \n",
    "    index_data(all_data)\n",
    "\n",
    "# Specify folders\n",
    "pdf_folder = \"./pdf1\"\n",
    "csv_folder = \"./pdf3CSV\"\n",
    "process_folder(pdf_folder, csv_folder)\n",
    "\n",
    "# Step 5: Search Functionality\n",
    "def search(query, top_k=5):\n",
    "    # Semantic Search (Chromadb)\n",
    "    query_embedding = openai_embeddings.embed_query(query)\n",
    "    chromadb_results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k\n",
    "    )\n",
    "\n",
    "    # Keyword Search (BM25 with Whoosh)\n",
    "    keyword_results = []\n",
    "    with ix.searcher() as searcher:\n",
    "        single_word_query = query.split()[0]\n",
    "        query_parser = QueryParser(\"content\", ix.schema)\n",
    "        whoosh_query = query_parser.parse(single_word_query)\n",
    "        print(f\"Whoosh query: {whoosh_query}\")\n",
    "\n",
    "        whoosh_results = searcher.search(whoosh_query, limit=top_k)\n",
    "        for hit in whoosh_results:\n",
    "            keyword_results.append({\n",
    "                \"content\": hit[\"content\"],\n",
    "                \"source\": hit[\"source\"],\n",
    "                \"page_or_row_num\": hit[\"page_or_row_num\"],\n",
    "                \"chunk_num\": hit[\"chunk_num\"],\n",
    "                \"score\": hit.score\n",
    "            })\n",
    "            print(f\"Found document: {hit['content'][:30]}... with score {hit.score}\")\n",
    "\n",
    "    results = {\n",
    "        \"semantic_results\": chromadb_results['documents'],\n",
    "        \"keyword_results\": keyword_results\n",
    "    }\n",
    "    return results\n",
    "\n",
    "# Sample query search\n",
    "query = \"Example query text\"\n",
    "results = search(query)\n",
    "print(\"Semantic Results:\", results[\"semantic_results\"])\n",
    "print(\"Keyword Results:\", results[\"keyword_results\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daun/miniconda3/envs/rag/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS 인덱스가 저장되었습니다.\n",
      "메타데이터가 저장되었습니다.\n",
      "FAISS 인덱스를 성공적으로 불러왔습니다.\n",
      "메타데이터를 성공적으로 불러왔습니다.\n",
      "Semantic Results: [{'content': '외야 펜스가 상대적으로 낮거나 가까워 홈런이 나기 쉬운 구조를 가지고 있습니다. 이는 \\n타자들이 더 쉽게 점수를 낼 수 있는 환경을 제공합니다.\"} \\n \\n{\"inputs\": \"선수들이 경기 할 때는 개인 경기도구를 사용해도 돼?\", \"response\": \"KBO 리\\n그의 경우, 개인 장비를 사용할 수 있지만 대회 사무국이 공인한 제품만 사용이 가능합\\n니다.\"} \\n{\"inputs\": \"보크가 뭐야?\", \"response\": \"주자가 있을 때 투수가 저지르는 변칙적인 투구동\\n작입니다.\"}', 'source': './pdf1/야구 지식.pdf', 'page_or_row_num': 3, 'chunk_num': 2}, {'content': '기의 변수를 창출해 내는 능력이 필요합니다.\"} \\n \\n \\n{\"inputs\": \"클린업 트리오는 어떤 역할을 해?\", \"response\": \"3, 4, 5번 타자를 일컫는 말로, \\n보통 팀 내 최고의 선수들을 이 타선에 배치하며 야구에서 득점할 때 가장 중요한 역할\\n을 하는 타선입니다. 미국에서는 4번 타자만을 얘기하지만 일본에서 3, 4, 5를 한꺼번에 \\n일컬으면서 4번 타자가 그 중에서도 팀의 상징이라는 것으로 변질됐습니다.\"}', 'source': './pdf1/야구 지식.pdf', 'page_or_row_num': 6, 'chunk_num': 1}, {'content': '작입니다.\"} \\n{\"inputs\": \"인필드플라이가 뭐야?\", \"response\": \"무사나 1사 상황에서 1, 2루 혹은 만루 상\\n황에 놓여있을 때, 타자가 내야수가 정상적인 수비로 잡을 수 있는 페어 플라이 볼을 치\\n는 것으로, 심판이 이것을 선언하면 수비수가 공을 잡는 것과 관계없이 타자는 아웃이 \\n됩니다.\"} \\n{\"inputs\": \"안타가 뭐야?\", \"response\": \"수비수의 실책이 없는 상태에서 최소한 타자가 한 \\n베이스(루) 이상을 갈 수 있게 공을 치는 것입니다.\"}', 'source': './pdf1/야구 지식.pdf', 'page_or_row_num': 3, 'chunk_num': 3}, {'content': '있으니 미리 확인하는 것이 좋습니다.\"} \\n{\"inputs\": \"야구장에 가져갈 수 있는 물건이 제한돼?\", \"response\": \"대부분의 야구장은 큰 \\n가방이나 위험 물품의 반입을 제한합니다. 작은 가방이나 배낭은 허용되나, 경기장에 따\\n라 다를 수 있으므로 방문 전 야구장 규정을 확인하는 것이 좋습니다.\"} \\n{\"inputs\": \"경기 중에 일어날 수 있는 특별한 상황이 뭐가 있어?\", \"response\": \"더블 플레\\n이, 트리플 플레이, 그랜드슬램 홈런 등 다양한 특별한 상황이 있습니다. 더블 플레이는', 'source': './pdf1/야구 지식.pdf', 'page_or_row_num': 1, 'chunk_num': 4}, {'content': '하나입니다.\"} \\n{\"inputs\": \"피안타, 피홈런이 뭐야?\", \"response\": \"각각 투수가 안타와 홈런을 맞았다는 의\\n미입니다.\"} \\n{\"inputs\": \"무승부가 나는 경우는 언제야?\", \"response\": \"한국의 경우, 12회말까지 두 팀의 \\n점수가 똑같으면 무승부로 인정합니다.\"} \\n{\"inputs\": \"범타가 뭐야?\", \"response\": \"타자가 친 타구가 안타가 되지 못하고 내야수나 \\n외야수의 글러브에 들어가 아웃된 것을 일컫습니다.\"}', 'source': './pdf1/야구 지식.pdf', 'page_or_row_num': 4, 'chunk_num': 4}]\n",
      "Keyword Results: []\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import faiss  # FAISS 가져오기\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from whoosh.index import create_in, open_dir\n",
    "from whoosh.fields import Schema, TEXT, ID\n",
    "from whoosh.qparser import QueryParser\n",
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle  # 메타데이터 저장용\n",
    "import uuid  # 고유 ID 생성을 위해\n",
    "\n",
    "# OpenAI API 키 설정\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"yourkey\"\n",
    "\n",
    "# OpenAI Embeddings 초기화\n",
    "openai_embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# FAISS 인덱스 초기화\n",
    "embedding_dim = 1536  # `text-embedding-ada-002` 모델의 차원\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "faiss_ids = []\n",
    "metadata_store = {}\n",
    "\n",
    "# 텍스트를 분할하는 함수 정의\n",
    "def chunk_text(text, chunk_size=300, chunk_overlap=50):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return text_splitter.split_text(text)\n",
    "\n",
    "# PDF에서 텍스트 추출 후 분할\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text_data = []\n",
    "    for page_num in range(doc.page_count):\n",
    "        page = doc[page_num]\n",
    "        text = page.get_text(\"text\")\n",
    "        chunks = chunk_text(text)\n",
    "        for chunk_num, chunk in enumerate(chunks, 1):\n",
    "            text_data.append({\n",
    "                \"content\": chunk,\n",
    "                \"page_num\": page_num + 1,\n",
    "                \"chunk_num\": chunk_num,\n",
    "                \"source\": pdf_path\n",
    "            })\n",
    "    return text_data\n",
    "\n",
    "# CSV 파일에서 데이터 추출\n",
    "def extract_data_from_csv(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    text_data = []\n",
    "    for i, row in df.iterrows():\n",
    "        row_text = row.to_string()\n",
    "        text_data.append({\n",
    "            \"content\": row_text,\n",
    "            \"row_num\": i + 1,\n",
    "            \"source\": csv_path\n",
    "        })\n",
    "    return text_data\n",
    "\n",
    "# Whoosh로 BM25 인덱스 초기화\n",
    "schema = Schema(content=TEXT(stored=True), source=ID(stored=True), page_or_row_num=ID(stored=True), chunk_num=ID(stored=True))\n",
    "index_dir = \"indexdir1107\"\n",
    "if os.path.exists(index_dir):\n",
    "    import shutil\n",
    "    shutil.rmtree(index_dir)\n",
    "os.mkdir(index_dir)\n",
    "ix = create_in(index_dir, schema)\n",
    "\n",
    "# 데이터 인덱싱 함수\n",
    "def index_data(data):\n",
    "    with ix.writer() as writer:\n",
    "        for entry in data:\n",
    "            unique_id = str(uuid.uuid4())\n",
    "            \n",
    "            # FAISS용 임베딩\n",
    "            embedding = openai_embeddings.embed_query(entry[\"content\"])\n",
    "            embedding_np = np.array([embedding], dtype=\"float32\")\n",
    "            \n",
    "            # 메타데이터에 'content' 추가\n",
    "            metadata = {\n",
    "                \"content\": entry[\"content\"],  # content 추가\n",
    "                \"source\": entry[\"source\"], \n",
    "                \"page_or_row_num\": entry[\"page_num\" if \"page_num\" in entry else \"row_num\"],\n",
    "                \"chunk_num\": entry.get(\"chunk_num\", 1)\n",
    "            }\n",
    "            \n",
    "            faiss_ids.append(unique_id)\n",
    "            metadata_store[unique_id] = metadata\n",
    "            index.add(embedding_np)  # FAISS 인덱스에 추가\n",
    "\n",
    "            # Whoosh BM25 인덱싱\n",
    "            writer.add_document(\n",
    "                content=entry[\"content\"],\n",
    "                source=entry[\"source\"],\n",
    "                page_or_row_num=str(entry[\"page_num\" if \"page_num\" in entry else \"row_num\"]),\n",
    "                chunk_num=str(entry.get(\"chunk_num\", 1))\n",
    "            )\n",
    "\n",
    "# 폴더 내 PDF 및 CSV 파일 처리\n",
    "def process_folder(pdf_folder, csv_folder):\n",
    "    all_data = []\n",
    "    for filename in os.listdir(pdf_folder):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(pdf_folder, filename)\n",
    "            pdf_data = extract_text_from_pdf(pdf_path)\n",
    "            all_data.extend(pdf_data)\n",
    "    \n",
    "    for filename in os.listdir(csv_folder):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            csv_path = os.path.join(csv_folder, filename)\n",
    "            csv_data = extract_data_from_csv(csv_path)\n",
    "            all_data.extend(csv_data)\n",
    "    \n",
    "    index_data(all_data)\n",
    "\n",
    "# FAISS 인덱스와 메타데이터 저장 함수\n",
    "def save_faiss_index(index, faiss_ids, metadata_store, index_path=\"faiss_index1107.bin\", metadata_path=\"metadata_store1107.pkl\"):\n",
    "    faiss.write_index(index, index_path)\n",
    "    print(\"FAISS 인덱스가 저장되었습니다.\")\n",
    "    \n",
    "    # 메타데이터 저장\n",
    "    with open(metadata_path, \"wb\") as f:\n",
    "        pickle.dump((faiss_ids, metadata_store), f)\n",
    "    print(\"메타데이터가 저장되었습니다.\")\n",
    "\n",
    "# FAISS 인덱스와 메타데이터 불러오기 함수\n",
    "def load_faiss_index(index_path=\"faiss_index1107.bin\", metadata_path=\"metadata_store1107.pkl\"):\n",
    "    index = faiss.read_index(index_path)\n",
    "    print(\"FAISS 인덱스를 성공적으로 불러왔습니다.\")\n",
    "    \n",
    "    # 메타데이터 불러오기\n",
    "    with open(metadata_path, \"rb\") as f:\n",
    "        faiss_ids, metadata_store = pickle.load(f)\n",
    "    print(\"메타데이터를 성공적으로 불러왔습니다.\")\n",
    "    \n",
    "    return index, faiss_ids, metadata_store\n",
    "\n",
    "# 폴더 경로 설정 및 인덱싱 수행 후 저장\n",
    "pdf_folder = \"./pdf1\"\n",
    "csv_folder = \"./pdf3CSV\"\n",
    "process_folder(pdf_folder, csv_folder)\n",
    "save_faiss_index(index, faiss_ids, metadata_store)\n",
    "\n",
    "# 검색 함수\n",
    "def search(query, top_k=5):\n",
    "    # 저장된 인덱스와 메타데이터 불러오기\n",
    "    index, faiss_ids, metadata_store = load_faiss_index()\n",
    "    \n",
    "    # FAISS 유사도 검색\n",
    "    query_embedding = np.array([openai_embeddings.embed_query(query)], dtype=\"float32\")\n",
    "    _, faiss_indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    semantic_results = []\n",
    "    for idx in faiss_indices[0]:\n",
    "        if idx < len(faiss_ids):\n",
    "            unique_id = faiss_ids[idx]\n",
    "            metadata = metadata_store[unique_id]\n",
    "            semantic_results.append({\n",
    "                \"content\": metadata[\"content\"],\n",
    "                \"source\": metadata[\"source\"],\n",
    "                \"page_or_row_num\": metadata[\"page_or_row_num\"],\n",
    "                \"chunk_num\": metadata[\"chunk_num\"]\n",
    "            })\n",
    "\n",
    "    # Whoosh로 키워드 검색\n",
    "    keyword_results = []\n",
    "    with ix.searcher() as searcher:\n",
    "        query_parser = QueryParser(\"content\", ix.schema)\n",
    "        whoosh_query = query_parser.parse(query)\n",
    "        whoosh_results = searcher.search(whoosh_query, limit=top_k)\n",
    "        for hit in whoosh_results:\n",
    "            keyword_results.append({\n",
    "                \"content\": hit[\"content\"],\n",
    "                \"source\": hit[\"source\"],\n",
    "                \"page_or_row_num\": hit[\"page_or_row_num\"],\n",
    "                \"chunk_num\": hit[\"chunk_num\"],\n",
    "                \"score\": hit.score\n",
    "            })\n",
    "\n",
    "    return {\n",
    "        \"semantic_results\": semantic_results,\n",
    "        \"keyword_results\": keyword_results\n",
    "    }\n",
    "\n",
    "# 샘플 검색\n",
    "query = \"Example query text\"\n",
    "results = search(query)\n",
    "print(\"Semantic Results:\", results[\"semantic_results\"])\n",
    "print(\"Keyword Results:\", results[\"keyword_results\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up OpenAI API key using environment variables\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"yourkey\"\n",
    "\n",
    "# pdf_folder = \"./pdf1\"\n",
    "# csv_folder = \"./pdf3CSV\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS 인덱스를 성공적으로 불러왔습니다.\n",
      "메타데이터를 성공적으로 불러왔습니다.\n",
      "Semantic Results: [{'content': '면 한번은 뛰어야 하는 팀’으로 인식될 정도였다.\\n여담으로 자매 팀과는 정반대다. 응원만큼은 롯데 자이언츠만큼이나 열정적이나, 일본에서 \\n가장 인기 없는 야구팀 중 하나로 꼽힌다. 다만 성적만큼은 치바 쪽이 좋은 편이다. 특히 한국 \\n롯데에겐 아직도 먼 21세기 우승을 치바는 두 번이나 해냈다(2005, 2010). 더욱이 2005년\\n도 우승은 당시 상대 팀에게 씻을 수 없는 모욕감을 선사했다.\\n부산경남 지역 민영방송인 KNN 라디오에서는 롯데 자이언츠 전 경기 생중계를 한다. 수요', 'source': './pdf1/롯데.pdf', 'page_or_row_num': 7, 'chunk_num': 3}, {'content': '경기 많은 홈경기를 치뤘으며, 홈 최종전까지 3위 싸움을 하는 박진감 넘치는 순위 싸움을 했\\n음에도, 최종적으로 홈 96만 관중 기록에 그치며 5년 만의 100만 관중 달성에 실패했다.\\n이는 야구를 처음 접한 사람들이 단순히 두산의 성적이 좋다는 이유로 응원을 하다가, 성적\\n이 떨어지면 야구에 대한 관심을 끊는 등 팬심이 상대적으로 끈끈하지 않다는 것을 증명한\\n다. 그럼에도, 2014년 새로 부임한 응원단장 한재권이 엄청난 능력을 보여주면서 무려 외야', 'source': './pdf1/두산.pdf', 'page_or_row_num': 6, 'chunk_num': 4}, {'content': '진영의 빈 공간으로 떨어뜨리는 쪽이 점수를 획득한다. 그러나 야구에서는 투수가 포수에게 \\n던지는 공을 상대팀 타자가 방망이로 쳐내야 하며, 아무리 공을 잘, 많이 쳐내도 주자가 홈 베\\n이스를 밟지 못하면 점수가 나지 않으며, 이런 식으로 아웃 카운트 세 개가 모두 잡힐 때까지 \\n홈에 들어오지 못한 주자를 잔루라 한다.\\n즉, 다른 구기가 공을 다루는 기술에 역점을 두어 발전해 왔다면, 야구는 좀 더 다양한 규칙과 \\n변수를 허용하는 형태의 엔터테인먼트로 발전해 왔다고 할 수 있다. 야구해설자 하일성은 \"', 'source': './pdf1/아구_나무위키.pdf', 'page_or_row_num': 5, 'chunk_num': 2}, {'content': '변수를 허용하는 형태의 엔터테인먼트로 발전해 왔다고 할 수 있다. 야구해설자 하일성은 \"\\n(야구의 매력 중 하나로) 다른 종목은 공이 득점을 하는 경기인데, 야구는 사람이 득점을 하\\n는 경기다.\"라는 말로 표현했다.\\n하지만 이런 점은 공만 있으면 간단하게 즐길 수 있는 여타 구기들과 비교하면 꽤나 까다롭\\n게 작용한다. 공뿐만 아니라 배트와 글러브를 기본으로 준비해야 하는 등 즐기기 위해서는 \\n여러 장비가 필요하다. 어떻게 구색을 갖춰서 시작한다 하더라도 경기 룰 자체가 복잡하고', 'source': './pdf1/아구_나무위키.pdf', 'page_or_row_num': 5, 'chunk_num': 3}, {'content': '야구-나무위키_문장\\n15\\n그러나 한국에서는 일반인들의 대상으로 야구에 대한 호감도가 조금 떨어진 듯한 경향이 있\\n는데, 여러 이유가 있겠지만, 일부 선수들의 낮은 팬서비스 수준과 비교적 어려운 규칙이 예\\n시로 꼽힌다. 국내 인기 스포츠 중 하나인 축구와 농구의 경우, 야구에 비해 간단한 규칙을 가\\n졌다고 생각하는 사람들이 많고, 야구를 잘 모르는 일반인들의 경우 이대호와 같이 슬러거형 \\n타자의 체형을 두고 \"운동을 안하니까 저런 몸매로 운동선수를 한다\"와 같은 오해를 갖고 있', 'source': './pdf1/아구_나무위키.pdf', 'page_or_row_num': 15, 'chunk_num': 1}]\n",
      "Keyword Results: []\n"
     ]
    }
   ],
   "source": [
    "# Sample query search\n",
    "query = \"롯데 야구 잘하는 것 같애 못 하는 것 같애\"\n",
    "results = search(query)\n",
    "print(\"Semantic Results:\", results[\"semantic_results\"])\n",
    "print(\"Keyword Results:\", results[\"keyword_results\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
